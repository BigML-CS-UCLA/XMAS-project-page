<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Contrastive Learning, Theory, Class Collapse, Feature Suppression">
  <meta property="og:title" content="Data Selection for Fine-Tuning Vision Language Models via Cross Modal Attention Trajectories"/>
  <meta property="og:description" content="Theoretical paper on data selection for efficient fine-tuning of Large Vision Language Models"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>


  <meta name="twitter:title" content="Data Selection for Fine-Tuning Vision Language Models via Cross Modal Attention Trajectories">
  <meta name="twitter:description" content="Theoretical paper on data selection for efficient fine-tuning of Large Vision Language Models">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="./static/images/not_all.png" class="is-centered">
  <meta name="twitter:card" content="./static/images/not_all.png" class="is-centered">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Data Selection for Fine-Tuning Vision Language Models via Cross Modal Attention Trajectories</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Data Selection for Fine-Tuning Vision Language Models via Cross Modal Attention Trajectories</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://hsgser.github.io/" target="_blank">Dang Nguyen</a>,
                </span>
                <span class="author-block">
                    <a href="https://nilayn7.github.io/" target="_blank">Nilay Naharas</a>,
                </span>
                <span class="author-block">
                  Nesihan Bulut,
                </span>
                <span class="author-block">
                  Mohammadhossein Bateni,
                </span>
                <span class="author-block">
                  Vahab Mirrokni,
                </span>
                <span class="author-block">
                  <a href="https://baharanm.github.io/" target="_blank">Baharan Mirzasoleiman</a>,
                </span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">UCLA CS</span>
                  </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                <!-- ArXiv abstract Link> 
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
       Visualization of the kinds of points we pick!
    </div>
  </div>
</section>
End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Data-efficient learning aims to eliminate redundancy in large training datasets by train- ing models on smaller subsets of the most informative examples. While data selection has been extensively explored for vision models and large language models (LLMs), it remains underexplored for Large Vision-Language Models (LVLMs). Notably, none of existing methods can outperform random selection at different subset sizes. In this work, we propose the first principled method for data-efficient instruction tuning of LVLMs. We prove that examples with similar cross-modal attention matrices during instruction tun- ing have similar gradients. Thus, they influence model parameters in a similar manner and convey the same information to the model during training. Building on this insight, we propose XMAS, which clusters examples based on the trajectories of the top singu- lar values of their attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a balanced subset from these clusters, XMAS effectively removes redundancy in large-scale LVLM training data. Extensive experiments show that XMAS can discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while fully preserving per- formance of LLaVA-1.5-7B on 10 downstream benchmarks and speeding up its training by 1.2×. This is 30% more data reduction compared to the best baseline for LLaVA-665k.

          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<br>
<br>
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <h2 class="title is-3">Method</h2>
      <hr>
      <div class="content has-text-justified">
        <img src="./static/images/xmas.png" class="is-centered">
        <p>
          <b> XMAS employs a small proxy VLM to find cross-modal alignment trajectories for examples in the fine-tuning data. Examples with similar cross-modal alignment trajectories have similar gradients during instruction tuning. Then, it clusters the cross-modal alignment trajectories and then samples a balanced subset of examples with more stable trajectories from the clusters to get the most repreentative subset for a given budget.</b>
        </p>
        <!-- <img src="./static/images/cc_gd.png" class="is-centered"> -->
      </div>
  </div>
</div>

  
<br>
<br>
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <h2 class="title is-3">Main Results</h2>
      <hr>
      <div class="content has-text-justified">
        <img src="./static/images/arp_results.png" class="is-centered">
        <p>
          1. <b> The average relative performance (ARP) of different subsets of (left) LLaVA 665k and (right) Vision-Flan when fine-tuning LLaVA-1.5-7B. Methods that outperform random selection are shown in opaque. XMAS is the only method that surpasses random selection across different budgets on both datasets. MP and HL finetune the target model on full data to find subsets and thus do not yield any speedup.</b>
        </p>
        <br>
        <img src="./static/images/xmas_combined_results.png" class="is-centered">
        <p>
          2. <b>  (left) Data reduction to reach 100% Average relative performance (ARP) of LLaVA-1.5-7B on LLaVA 665k. XMAS obtains 30% more data reduction over the best baselines. (middle) ARP ranking of different 10% subsets of LLaVA 665k when fine-tuning LLaVA-1.5-13B. (right) ARP and ratio of total time (selection + training) w.r.t training target model on full dataset for XMAS at different budgets on LLaVA 665k. XMAS reduces training time by a factor of 0.84 (1.2× speedup) to reach 100% ARP. </b>
        </p>
        <!-- <img src="./static/images/cc_gd.png" class="is-centered"> -->
      </div>
  </div>
</div>

<!-- Image carousel 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
 End image carousel -->




<!-- Youtube video 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
End youtube video -->

<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @InProceedings{pmlr-v202-xue23d,
          title = 	 {Which Features are Learnt by Contrastive Learning? {O}n the Role of Simplicity Bias in Class Collapse and Feature Suppression},
          author =       {Xue, Yihao and Joshi, Siddharth and Gan, Eric and Chen, Pin-Yu and Mirzasoleiman, Baharan},
          booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
          pages = 	 {38938--38970},
          year = 	 {2023},
          editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
          volume = 	 {202},
          series = 	 {Proceedings of Machine Learning Research},
          month = 	 {23--29 Jul},
          publisher =    {PMLR},
          pdf = 	 {https://proceedings.mlr.press/v202/xue23d/xue23d.pdf},
          url = 	 {https://proceedings.mlr.press/v202/xue23d.html},
          abstract = 	 {Contrastive learning (CL) has emerged as a powerful technique for representation learning, with or without label supervision. However, supervised CL is prone to collapsing representations of subclasses within a class by not capturing all their features, and unsupervised CL may suppress harder class-relevant features by focusing on learning easy class-irrelevant features; both significantly compromise representation quality. Yet, there is no theoretical understanding of <em>class collapse</em> or <em>feature suppression</em> at <em>test</em> time. We provide the first unified theoretically rigorous framework to determine <em>which</em> features are learnt by CL. Our analysis indicate that, perhaps surprisingly, bias of (stochastic) gradient descent towards finding simpler solutions is a key factor in collapsing subclass representations and suppressing harder class-relevant features. Moreover, we present increasing embedding dimensionality and improving the quality of data augmentations as two theoretically motivated solutions to feature suppression. We also provide the first theoretical explanation for why employing supervised and unsupervised CL together yields higher-quality representations, even when using commonly-used stochastic gradient methods.}
        }
        
      </code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


  </body>
  </html>
